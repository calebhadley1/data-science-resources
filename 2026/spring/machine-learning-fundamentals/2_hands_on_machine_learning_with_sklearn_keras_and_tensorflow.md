# [Hands-on machine learning with Scikit-Learn, Keras and TensorFlow: concepts, tools, and techniques to build intelligent systems](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)
Géron, Aurélien

## Chapter 1: The Machine Learning Landscape
- The first application of ML, spam filters, is used as an example for why ML should be used. Instead of maintaining a complicated list of spam filters, you can let ML learn the patterns for you
- ML is great for (p. 7):
    - Problems requiring complicated or long lists of rules
    - Fluctuating environments (re-training is easier than changing all your code rules)
    - Getting new insights about complex problems and large amounts of data
- Pages 8/9 outline a ton of different use cases for ML
- Types of ML Systems
    - Unsupervised, semi-supervised, self-supervised, and others
    - Incremental vs on the fly learning (online vs batch learning)
    - Do they compare new data points to known ones? Or detect patterns in training data and build a predictive model?
- Training Supervision
    - Supervised learning includes labels
    - Unsupervised learning uses unlabeled data (dimensionality reduction, clustering, anomaly detection)
    - Semi-supervised uses partially labeled data (Think when you upload photos of your family to a photo app. At the start they are unlabeled, so clustering is used, then once a label is assigned to one it knows all the others are the same)
    - Self-supervised. Generating labeled dataset from unlabeled one. An image based example is provided on page 15
    - Reinforcement learning uses rewards and penalties to get an agent to reach the best strategy (policy). 
- Batch vs Online Learning
    - Batch
        - Model is trained once and deployed into production to utilize its learnings
        - Can suffer from model rot/data drift
        - If you have a huge dataset to train on and data frequently changes, batch is probably not a good fit due to training cost
    - Online
        - System is trained incrementally by feeding data individually or in mini batches
        - This can help when training set is too large for the memory of the machine training the model (p. 19)
        - Learning rate is important to decide on for this learning type
        - One drawback is that new data can corrupt a model without you knowing. You need good monitoring
- Instance Based vs Model Based Learning
    - Instance (p. 21)
        - Models like KNN use a similarity metric to see how close an instance is to other ones it was trained on
        - The model learns examples by heart and generalizes new examples based on the prior ones
    - Model (p. 22)
        - Model of the training examples is built and predictions are made using that model
    - Python Snippets for the KNN / Reg models are provided on pages 23 - 25
## Chapter 4: Training Models (Or at least up to page 154)
- This chapter dives deeper into the black box that is a "model". It discusses deriving the closed form equation and utilizing gradient descent to find the optimal parameters
- Linear Regression is formalized on page 132
- The Normal Equation, the closed form solution for Linear Regression, and the math to calculate it is discussed on page 135. It turns out that calculating the normal equation is more expensive than utilizing gradient descent. 
- Gradient Descent, a method for finding optimal solutions for a number of different problems, is introduced on page 138
    - Random parameter initialization
    - Learning rate, and the consequences of choosing too high or low of a rate (p. 139-140)
    - Feature scaling and how it affects gradient descent (p. 141)
    - Varieties of gradient descent
        - Batch (p. 142)
        - Stochastic (p. 145) is less regular than Batch, but it can jump out of local minima easier
        - Mini-Batch (p. 148) calculates gradients against random subsets of the training set instead of the entire set
- Polynomial Regression (p. 149) allows us to model more complicated relationships
    - Learning rates (p. 152-153) should be examined to ensure models are not over/underfitting
