# [Data Science for Business: What You Need to Know About Data Mining and Data-Analytic Thinking](https://www.academia.edu/38731456/Data_Science_for_Business)

This book mainly covers case studies in Data Science across different business domains. It shows how DS can make an impact on business
and how data should be viewed as a business asset.

## Ch 1: Introduction to Data Analytic Thinking

Chapter 1: Introduction to Data Analytic Thinking
- The volume of data has surpassed the ability for statisticians and models to manually inspect data and derive insights. To address this Data Science has risen in popularity.
- Case Study 1: Hurricane Frances
    - Walmart utilizes DS to predict which items would be in demand for the hurricane. It came to counter-intuitive realizations that items such as strawberry PopTarts would be in demand. 
- Case Study 2: Customer Churn
    - MegaTelCo uses DS to identify which customers are likely to churn. By doing this they can target these customers with marketing campaigns. 
- Data Driven Decision Making (DDD) - The practice of basing decisions on the outcome of data analysis rather than intuition. A study on DDD @ MIT is covered to support its use. 
- Case Study 3: Target Pregnancy Predictions
    - Target and many other retailers compete to win the business of families which are welcoming new children. They know that children result in the buying habits changing (where you buy diapers is where you buy everything). Target had the realization that if it can predict *who* is pregnant, they can win their business before anyone even knows that the family has a new child. 
- For both case studies 1 & 3, the data is explored without a simple hypothesis with the hope that something useful can be uncovered. Case study 2 however has a basis hypothesis, predict who will churn. (Type 1 vs 2 DDD problems).
- Big Data 1.0 vs 2.0. First companies will learn how to process data (setup the plumbing). In 2.0, business will uncover all the different ways they can leverage this data.
- **Data should be viewed as a strategic asset** - Signet Bank to Capital One story. 
- Cross Industry Process for Data Mining (CRISP-DM).
- Data Science vs the Tools of Data Science (Analogy of how Chemists are not just the test tubes. Likewise Data Scienctists are not just Python, Pandas, Numpy etc. It is the science of discovering trends from data that is important)

## Ch 2: Business Problems and Data Science Solutions

Chapter 2: Business Problems in Data Science:
- Data mining is a *process* with fairly well defined stages
- “Each data driven business decision making problem is unique, comprising its own combination of goals, desires, constraints and even personalities. As with much engineering, though, there are sets of common tasks that underlie the business problems. In collaboration with business stakeholders, data scientists decompose a business problem into subtasks.”
- Types of problems:
    - “*Classification* and class *probability* *estimation* attempt to predict, for each individual in a population, which of a (small) set of classes this individual belongs to.”
        - Classes are usually mutually exclusive
        - Ex: Among all MegaTelCo customers, which are likely to response to a given offer? Classes are “will respond” and “will not respond”
        - Closely related tasks are *scoring* or class *probability estimation* (score representing the likelihood an individual belongs to a class)
        - Fundamentally predicts *whether* something will happen
    - Regression (*value estimation*)
        - “…estimate or predict, for each individual, the numerical value of some variable for that individual.
        - Ex: How much will a given customer use the service? What is the estimated income of an individual?
        - Fundamentally predicts *how much* something will happen
    - Similarity matching
        - *Identify* similar individuals based on data known about them
        - Ex: IBM wants to find companies similar to their best business customers in order to identify the best sales opportunities
        - This is the basis for one of the most popular methods for making product recommendations
        - Similarity measures underlie certain solutions to other data mining tasks such as classification, regression, and clustering. (Chapter 6)
    - Clustering
        - *Groups* individuals in a population together by their similarity, but not driven by any specific purpose
        - Ex: Do our customers naturally form groups or segments?
        - Useful in preliminary domain exploration to see which natural groups exist because these groups in turn may suggest other data mining tasks or approaches
        - Used as an input to decision making processes focus on questions like “What products should be offer or develop?”
    - Co-occurence grouping (frequent itemset mining, association rule discovery, and market basket analysis)
        - *Associations* between entities based on transactions involving them
        - Ex: What items are commonly purchased together?
        - Clustering vs co-occurence grouping:
            - Clustering looks at grouping between objects based on objects’ attributes
            - Co-occurence considers similarity of objects based on their appearing together in transactions
    - Profiling (behavior description)
        - Characterize behavior of an individual, group, or population
        - Ex: What is the typical cell phone usage of this customer segment?
        - Use case: Identify norms for anomoly detection applications like fraud detection
    - Link prediction
        - Predict connects between data items, usually by suggesting that a link should exist, and possibily also estimating the strength of the link
        - Common in social networking systems
        - Ex: You and Karen share 10 friends, maybe you want to be friends?
        - Can also estimate strength of a link
        - Forms basis of recommendation
    - Data reduction
        - Attempts to take a large set of data and replace it with a similar set of data that contains much of the important information in the larger set
        - Ex: A massive dataset on consumer viewing preferences may be reduced to a smaller one on consumer taste preferences
        - Involves loss of information. What is important is trade off for improved insight
    - Causal modeling
        - Attempts to help us understand what events or actions actually influence others
        - Ex: We use predictive modeling to target ads to consumers, and we observe the targeted consumers purchase at a higher rate. Was this because ads influenced consumer to purchase? Or did the model just predict who was going to buy anyways
        - Testing strategies include A/B tests/controlled experiments. Counterfactual analysis
        - ** In all cases a carful DS should always include with a causal conclusions the exact assumptions that must be made for it to hold
- Supervised vs unsupervised methods
    - Is a target being predicted? Then it is supervised
    - Supervised requires that there is data on the target available (how do you validate predictions without)
    - Supervised methods include classification, regression, causal modeling
    - Unsupervised methods include clustering, co-occurence grouping, profiling
    - Similarity matching, link prediction, and data reduction could be either
- Data Mining and Its Results
    - Mining data to find patterns and build models vs using the results
    - The latter is how the models are used in productions on new examples generally.
- Cross industry standard process for data mining (CRISP-DM)
- Business understanding
    - It is an iterative process to recast the problem statement and design a solution
    - The design team should first think carefully about the problem o be solved and the use case
- Data understanding
    - If solving the business problem is the goal, data comprises the available raw material to build the solution
    - Understand strengths/limitations of data. E.g. Historical data does generally not have the DS problem as its purpose for existence. Customer, transaction, and marketing DBs can contain difference information, intersection populations, and have varying reliability.
    - Data cost can be variable
    - Dataset might be missing data on your target variable
- Data preparation
    - Data type conversion (e.g. tabular)
    - Missing values
    - Data types
    - Normalization and scaling
    - Leaks (does data contain information on the target variable)
- Modeling
    - Preliminary stage where data mining techniques are applied to the data
    - Model or pattern capturing regularities of the data
- Evaluation
    - Assess data mining results rigorously and to gain confidence that they are valid and reliable before moving on
    - Ensure model satisfies original business goals e.g. does fraud detection solution produce too many false alarms (comprehensibility of the model through testing in prod like environment)
- Deployment
    - Put results of data mining into real world use
    - Increasing these are automated deployments of the data mining techniques. E.g. when new ad campaign is presented, systems are deployed that build and test models in production
        - Do this because the world is changing faster than the data science team can adapt or business has too many tasks for data science to handle
- Managing a Data Science Team
    - Do not think of CRISP-DM as the software engineering lifecycle. It is closer to research and development than engineering. It iterates on *approaches* and *strategy* rather than software designs. Outcomes are less certain and the findings from a step can. fundamentally change the understanding of the problem
    - “Engineering a data mining solution directly for deployment can be an expensive premature commitment. Instead … prepare to invest in information to reduce uncertainty”
        - Pilot studies
        - Throwaway prototypes
        - Review literature to see what else is done and how it has worked
- Other Analytics Techniques and Technologies
    - Statistics
        - Summary statistics vs the field of stats
    - Database Querying
        - SQL vs OLAP
    - Data Warehousing
        - Integration of distributed datasets (not always necessary)
    - Regression Analysis
    - Machine Learning and Data Mining
    - Answer business problems using the techniques/technologies outlined above. Refer to page 40-41

## Ch 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation
Chapter 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation
- "Fundamental concepts: Identifying informative attributes; Segmenting data by progressive attribute selection"
- Exemplary techniques: Finding correlations, Attribute/variable selection; Tree induction
- We think of predictive modeling as "Supervised Segmentation"-how can we segment the population into groups that differ from each  other with  respect to some quantity of interest. -> Segment in  terms of something we want to predict or estimate
- Target can be  cast negatively (something we want to prevent) or positively
- "information is a quantity  that reduces uncertainty about something"
- "Finding informative attributes  also is the basis for *tree induction*"
- Model accuracy vs intelligibility  trade off (Best model vs one that makes sense)
- Models, Induction and Prediction
    - "Supervised  learning is model creation where the model describes a relationship between a set of select  variables (attrs or features) and a predefined  variable called the target variable"
    - Model induction = creation of models from data
        - Induction is a term from philosophy "to generalize from specific cases to general rules"
        - Induction algorithm/learner = procedure which creates the model from data
- Supervised Segmentation
    - How can we judge if a variable contains important information about the target variable? How much?
        - One method is "direct, multivariate supervised segmentation" (in classification)
            - When segmenting, we would prefer "pure" segments (each instance in the segment have same val for target), in the real world its not realistic though ("homogenous w.r.t. target variable")
            - 1. Attrs rarely split a group purely.
            - 2. Is it better when a group splits more broadly or more purely?
            - 3. Not all attrs are binary. How do we compare  these?
            - 4. Can you segment numeric attrs?
            - You solve these questions using a *purity measure* for classification problems
            - Most common splitting criterion is *information gain*. It is based on a purity measure called *entropy* (Shannon, 1948)
            - Explanation of entropy and information gain (p 51-55)
            - Information gain is not the correct splitting criterion for regression, variance is better for numeric but is technically a purity measure
- Example: Attr Selection with Information Gain
    - Discusses using information gain to reduce the entropy of a mushroom dataset
- Supervised Segmentation with Tree Structured Models
    - Classification/decision trees (p 63). Similar concept to trees in programming. The leaf nodes represent the class to predict. The decision points descend based on information gain. The first one after the root gives your best information gain.
    - *"Tree induction"* - How to create a tree structured model
    - Advantages to trees:
        - Easy to understand, simple to describe, easy to use, robust to common data problems, relatively efficient
    - Goal of tree: "Partition instances based on their attrs into subgroups that havee similar values for their target variables"
    - "Divide and conquer approach" - create purest subgroups possible
    - When do you stop splitting? When there are no more variables or you chose to stop earlier (simpler tree, Chapter 5)
- Visualizing Segmentations
    - Visualize how a classification tree partitions the instance space
    - Common form of instance space visualization is the scatter plot (p 70)
- Trees as Sets of Rules
    - Trees can be interpreted as logical statements using IF AND THEN syntax (p 71)
- Probability Estimation
    - Sometimes we need a more informative prediction than classification. For example is one prediction a more strong yes/no than another? How likely is someone to churn? This is whree probability estimation comes in
    - We would like each "segment to be assigned an estimate of the probability of membership in the different classes" (p 72)
    - *probability estimation tree*
    - frequency based estimate of class membership probability (extends the previously built up example of segmenting with trees)
        - Can lead to overfitting when there are not many examples at a leaf node
        - You can use smoothing "*Laplace correction*", which moderates influence of leafs with few instances
- Example: Addressing the Churn Problem with Tree Induction
    - Before building the model, check how good each variable is independetly with information gain
    - Apply classification tree alg
    - Notice that the order of the tree decisions does not necessarily match the initial information gain rankings. This is because "Nodes in the clf tree depend on the instances abovee them in the tree"
    - When do you stop building the tree?
        - Generality and overfitting come in here (CH 5)
        - Stop before these happen
    - Now that we have a model and we test it on some data, do we trust it? This is where *model evaluation* comes in (CH 7/8)
- Summary (p 78/79)
    - Information Gain
    - Entropy
    - Feature Selection
    - Segmenting
    - Trees and their history/implmentations


## Ch 4: Fitting a Model to Data
- Fundamental concepts: finding optimal model parameters based on data. Choosing the goal for data mining (objective function, loss function)
- Exemplary techniques: Linear regression, logistic regression, support vector machines
- *Parameter learning* / *Parametric modeling*
    - A different approach to how we learned the supervised segmentation model. Instead of recursively splitting the instace space by finding informative attrs, we start by
        1. Specify structure of the model with certain param left unspecified
        2. Data mining calculates best param vaues given set of training data
        3. Params can be chosen by domain knowledge, or attr selection (ch 3 approach)
    - Fundamentally we chose the model and attrs and the goal of data mining is to tune the params so that the model fits the data best as possible without overfitting
    - An example of this are linear models
- Page 83-85 shows a great comparison between how trees versus linear models split the param space. Deicision trees get many horizontal/vertical "cuts", whereas the linear picks one but has the flexibility to draw horizontal, vertical, or a combination of the two (linear discriminant function). It can also be described as perpendicular cuts vs piecewise function (p 103)
    - Trees of rules versus numerical function
- Linear disciminant
    - The function of the decision boundary is a weighted sum of the attrs
    - P 88 shows that many linear boundaries can separate the groups of points. How do we best chose the line? This is where the objective function comes in
- Optimizing an Objective Function
    - The objective function represents our goal. We optimize the parameters to satisfy this goal. Therefore our parameters are only good if we truly believe the objective function represents what we want to achieve.
    - Finding a true representation is often impossible so we must rely on faith and experience
        - One is support vector machine
- An Example of Mining a Linear Discriminant from Data
    - P 89-91 shows how different models would fit the Iris dataset
- Linear Discriminant Functions for Scoring and Ranking Instances
    - Usually we dont want to just know whether an instance belongs to a class, we want to know *how likely* is belongs. One way to solve this is using the class probability estimation from Chapter 3 (prob from leaf nodes). There is another way with linear models.. logisitic regression
    - Another use case is targetted market (p. 91). We don't necessary need an exact probability like 0.123456, but a score will be good enough: 12.
- Support Vector Machines Briefly
    - SVM's objective function incorporates the idea that "wider is better."
    - We create a margin (p. 93) between the classes and try to optimize its width
    - It also has a method for understanding loss when instances fall on the wrong side of the line
    - Balance between margin width and low total error penalty
    - "Hinge loss"
- Regression
    - Regression has many possible objective functions, mean squared error, absolute error, etc. What do we want to penalize? Depends on business use case
- Class Probability Estimation and Logistic "Regression"
    - By chosing a different objective function for our linear model, we can build a model to provide accurate estimates of class probability. Most commonly we use logistic regression for this
    - Class probability estimates have a few considerations:
        - "Well calibrated" & "Discriminative" (p. 98)
    - Why use logistric reg instead of linear reg when determining class probability?
        - For linear reg, f(x) gives a range from -inf to inf distance from the separating boundary. Prob should range from 0-1
    - So how does log solve the infinity problem?
        - p. 98-99 outlines a correlary using log odds to describe probability
        - For log reg, let f(x) return the model's estimation of the log odds that x belongs to the positive class. Then we can use algebra to translate the log odds back to probability
    - Log reg is technically a class probability estimation model, not a regression model
- Example: Logistic Regression versus Tree Induction
    - Model explainability vs accuracy trade-off (p. 105-107)
- Nonlinear Functions, Support Vector Machines, and Neural Networks
    - Nonlinear SVMs and Neural networks are the most populat models that fit parameters of complex nonlinear funcs
    - Kernal functions (linear, polynomial, etc)
    - Explanation of Neural Nets as a stack of models (p 109)
        - Risk of fitting data too well
- Summary
    - Second type of predictive modeling technique called function fitting or parametric modeling
    - Linear reg
    - Log reg
    - SVM
    - Linear discriminants
    - Objective functions
    - Predictive perf vs intelligibility
    - Overfitting

## Chapter 5: Overfitting and Its Avoidance
- Fundamental concepts: Generalization, Fitting and overfitting, Complexity control
- Exemplary techniques: Cross-validation, Attribute selection, Tree pruning, Regularization
- Defines overfitting as "Finding chance occurences in data that look like interesting patterns, but which do not generalize well" (p. 111)
- Generalization
    - Defined as "The property of a model or modeling process, whereby the model applies to data that were not used to build the model." (p. 112).
    - We want the models to apply beyond the training dataset
- Overfitting
    - Defined as "The tendency of data mining procedures to tailor models to the training data, at the expense of generalization to previously unseen data points" (p. 113)
    - All data mining procedures have the tendency to overfit to some extend (some more than others)
    - "The answer is not to use a data mining procedure that doesn't overfit because all of them do. Nor is the answer to simply use models that produce less overfitting, because there is a fundamental trade-off between model complexity and the possibility of overfitting... The best strategy is to recognize overfitting and to manage complexity in a principled way." (p. 113)
- Overfitting Examined
    - Holdout Data and Fitting Graphs
        - Fitting graph shows the accuracy of a model as a function of complexity
        - To examine *overfitting*, we need to introduce a fundamental concept to evaluation in DS, *holdout* data
        - We hide certain instances from the model, get predictions on them, and compare against the hidden truth values
        - Shows on page 114 a fitting graph and the tradeoff between complexity, error rate, and holdout vs training data
    - Overfitting in Tree Induction
        - If you continually split nodes in the tree, eventually you will end up with a lookup table (perfectly accurate for training set)
        - This isn't ideal, but still not as bad as the previous example of an overfitted classifier, since even for new examples it will non-trivially give some prediction.
        - We can artifically limit the number of nodes to avoid overfitting
        - Page 117 shows the "sweet" spot when fitting for tree induction
    - Overfitting in Mathematical Functions
        - Mathematical fns can become more complex by adding more variables
        - This can give the modeling procedyre too much leeway to fit the training set
        - Manual feature selection where possible
- From Holdout Evaluation to Cross-Validation
    - Cross-validation is a more sophisticated technique for the holdout training and testing procedure (p. 126)
    - Computes mean and variance so that we can understand how performance is expected to vary across datasets
    - Makes better use of a limited dataset by computing its estimates across all the data (systematically swaps out samples for testing)
    - Split labeled dataset into k partitions called *folds* (usually 5-10)
    - Iterate training and testing k times in a particular way
        - Chose different fold as the test data each iteration
        - Produce a model (one estimate of generalization performance)
    - When it is done, we have used every example once for testing and k-1 times for training
    - Now you can estimate mean and std/var
- Learning Curves
    - Defined as "A plot of the generalization performance against the amount of training data" (p. 131).
    - Learning curves show generalization performance, whereas fitting graphs also shows performance on the training data plotting against model complexity.
    - Learning curves depend on the type of modeling procedure
        - Page 131-132 explains the difference of learning curves characteristics between logistic regression and tree induction
- Overfitting Avoidance and Complexity Control
    - "To avoid overfitting, we control the complexity of the models induced from the data" (p. 133).
    - Avoiding Overfitting with Tree Induction
        - Strategies to avoid overfitting:
            - Stop growing the tree before it gets to complex (limit splits)
            - Grow tree until it is too large, then "prune" it back, reducing its size
        - There are simple ways to complete these strategies:
            - Limit tree size by specifying min # of instances that must be present in a leaf
                - What threshold should be use?
                    - Use hypothesis testing (described in detail in p. 133)
            - Pruning
    - A General Method for Avoiding Overfitting
        - "More generally, if we have a collection of models with different complexities, we could choose the best simply by estimating the generalization performance of each" (p. 134)
        - "Nested holdout testing" and "Nested cross-vaalidation" is described pages 134-135
        - "Sequential forward selection" (SFS)
        - "Sequential backward elimination"
    - Avoiding Overfitting for Parameter Optimization
        - Feature selection can be used to ensure the correct set of model attributes is selected for linear models and also some non-linear models
        - *Regularization*: Instead of optimizing to fit the data, we optimize some combination of fit and simplicity. Models will be better if they fit the data better, but they will also be better if they are simpler
        - *ridge regression* and *lasso* (L1 vs L2 norm)
- Summary (p. 140)
    - Model complexity vs Overfitting
    - All models can overfit
    - Fitting graphs and learning curves
    - Cross-validation and holdout sets
    - Regularization

## Chapter 6: Similarity, Neighbors, and Clusters
- TODO fill this in