# [Data Science for Business: What You Need to Know About Data Mining and Data-Analytic Thinking](https://www.academia.edu/38731456/Data_Science_for_Business)

This book mainly covers case studies in Data Science across different business domains. It shows how DS can make an impact on business
and how data should be viewed as a business asset.

## Introduction to Data Analytic Thinking

Chapter 1: Introduction to Data Analytic Thinking
- The volume of data has surpassed the ability for statisticians and models to manually inspect data and derive insights. To address this Data Science has risen in popularity.
- Case Study 1: Hurricane Frances
    - Walmart utilizes DS to predict which items would be in demand for the hurricane. It came to counter-intuitive realizations that items such as strawberry PopTarts would be in demand. 
- Case Study 2: Customer Churn
    - MegaTelCo uses DS to identify which customers are likely to churn. By doing this they can target these customers with marketing campaigns. 
- Data Driven Decision Making (DDD) - The practice of basing decisions on the outcome of data analysis rather than intuition. A study on DDD @ MIT is covered to support its use. 
- Case Study 3: Target Pregnancy Predictions
    - Target and many other retailers compete to win the business of families which are welcoming new children. They know that children result in the buying habits changing (where you buy diapers is where you buy everything). Target had the realization that if it can predict *who* is pregnant, they can win their business before anyone even knows that the family has a new child. 
- For both case studies 1 & 3, the data is explored without a simple hypothesis with the hope that something useful can be uncovered. Case study 2 however has a basis hypothesis, predict who will churn. (Type 1 vs 2 DDD problems).
- Big Data 1.0 vs 2.0. First companies will learn how to process data (setup the plumbing). In 2.0, business will uncover all the different ways they can leverage this data.
- **Data should be viewed as a strategic asset** - Signet Bank to Capital One story. 
- Cross Industry Process for Data Mining (CRISP-DM).
- Data Science vs the Tools of Data Science (Analogy of how Chemists are not just the test tubes. Likewise Data Scienctists are not just Python, Pandas, Numpy etc. It is the science of discovering trends from data that is important)

## Business Problems and Data Science Solutions

Chapter 2: Business Problems in Data Science:
- Data mining is a *process* with fairly well defined stages
- “Each data driven business decision making problem is unique, comprising its own combination of goals, desires, constraints and even personalities. As with much engineering, though, there are sets of common tasks that underlie the business problems. In collaboration with business stakeholders, data scientists decompose a business problem into subtasks.”
- Types of problems:
    - “*Classification* and class *probability* *estimation* attempt to predict, for each individual in a population, which of a (small) set of classes this individual belongs to.”
        - Classes are usually mutually exclusive
        - Ex: Among all MegaTelCo customers, which are likely to response to a given offer? Classes are “will respond” and “will not respond”
        - Closely related tasks are *scoring* or class *probability estimation* (score representing the likelihood an individual belongs to a class)
        - Fundamentally predicts *whether* something will happen
    - Regression (*value estimation*)
        - “…estimate or predict, for each individual, the numerical value of some variable for that individual.
        - Ex: How much will a given customer use the service? What is the estimated income of an individual?
        - Fundamentally predicts *how much* something will happen
    - Similarity matching
        - *Identify* similar individuals based on data known about them
        - Ex: IBM wants to find companies similar to their best business customers in order to identify the best sales opportunities
        - This is the basis for one of the most popular methods for making product recommendations
        - Similarity measures underlie certain solutions to other data mining tasks such as classification, regression, and clustering. (Chapter 6)
    - Clustering
        - *Groups* individuals in a population together by their similarity, but not driven by any specific purpose
        - Ex: Do our customers naturally form groups or segments?
        - Useful in preliminary domain exploration to see which natural groups exist because these groups in turn may suggest other data mining tasks or approaches
        - Used as an input to decision making processes focus on questions like “What products should be offer or develop?”
    - Co-occurence grouping (frequent itemset mining, association rule discovery, and market basket analysis)
        - *Associations* between entities based on transactions involving them
        - Ex: What items are commonly purchased together?
        - Clustering vs co-occurence grouping:
            - Clustering looks at grouping between objects based on objects’ attributes
            - Co-occurence considers similarity of objects based on their appearing together in transactions
    - Profiling (behavior description)
        - Characterize behavior of an individual, group, or population
        - Ex: What is the typical cell phone usage of this customer segment?
        - Use case: Identify norms for anomoly detection applications like fraud detection
    - Link prediction
        - Predict connects between data items, usually by suggesting that a link should exist, and possibily also estimating the strength of the link
        - Common in social networking systems
        - Ex: You and Karen share 10 friends, maybe you want to be friends?
        - Can also estimate strength of a link
        - Forms basis of recommendation
    - Data reduction
        - Attempts to take a large set of data and replace it with a similar set of data that contains much of the important information in the larger set
        - Ex: A massive dataset on consumer viewing preferences may be reduced to a smaller one on consumer taste preferences
        - Involves loss of information. What is important is trade off for improved insight
    - Causal modeling
        - Attempts to help us understand what events or actions actually influence others
        - Ex: We use predictive modeling to target ads to consumers, and we observe the targeted consumers purchase at a higher rate. Was this because ads influenced consumer to purchase? Or did the model just predict who was going to buy anyways
        - Testing strategies include A/B tests/controlled experiments. Counterfactual analysis
        - ** In all cases a carful DS should always include with a causal conclusions the exact assumptions that must be made for it to hold
- Supervised vs unsupervised methods
    - Is a target being predicted? Then it is supervised
    - Supervised requires that there is data on the target available (how do you validate predictions without)
    - Supervised methods include classification, regression, causal modeling
    - Unsupervised methods include clustering, co-occurence grouping, profiling
    - Similarity matching, link prediction, and data reduction could be either
- Data Mining and Its Results
    - Mining data to find patterns and build models vs using the results
    - The latter is how the models are used in productions on new examples generally.
- Cross industry standard process for data mining (CRISP-DM)
- Business understanding
    - It is an iterative process to recast the problem statement and design a solution
    - The design team should first think carefully about the problem o be solved and the use case
- Data understanding
    - If solving the business problem is the goal, data comprises the available raw material to build the solution
    - Understand strengths/limitations of data. E.g. Historical data does generally not have the DS problem as its purpose for existence. Customer, transaction, and marketing DBs can contain difference information, intersection populations, and have varying reliability.
    - Data cost can be variable
    - Dataset might be missing data on your target variable
- Data preparation
    - Data type conversion (e.g. tabular)
    - Missing values
    - Data types
    - Normalization and scaling
    - Leaks (does data contain information on the target variable)
- Modeling
    - Preliminary stage where data mining techniques are applied to the data
    - Model or pattern capturing regularities of the data
- Evaluation
    - Assess data mining results rigorously and to gain confidence that they are valid and reliable before moving on
    - Ensure model satisfies original business goals e.g. does fraud detection solution produce too many false alarms (comprehensibility of the model through testing in prod like environment)
- Deployment
    - Put results of data mining into real world use
    - Increasing these are automated deployments of the data mining techniques. E.g. when new ad campaign is presented, systems are deployed that build and test models in production
        - Do this because the world is changing faster than the data science team can adapt or business has too many tasks for data science to handle
- Managing a Data Science Team
    - Do not think of CRISP-DM as the software engineering lifecycle. It is closer to research and development than engineering. It iterates on *approaches* and *strategy* rather than software designs. Outcomes are less certain and the findings from a step can. fundamentally change the understanding of the problem
    - “Engineering a data mining solution directly for deployment can be an expensive premature commitment. Instead … prepare to invest in information to reduce uncertainty”
        - Pilot studies
        - Throwaway prototypes
        - Review literature to see what else is done and how it has worked
- Other Analytics Techniques and Technologies
    - Statistics
        - Summary statistics vs the field of stats
    - Database Querying
        - SQL vs OLAP
    - Data Warehousing
        - Integration of distributed datasets (not always necessary)
    - Regression Analysis
    - Machine Learning and Data Mining
    - Answer business problems using the techniques/technologies outlined above. Refer to page 40-41

## Introduction to Predictive Modeling: From Correlation to Supervised Segmentation
Chapter 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation
- "Fundamental concepts: Identifying informative attributes; Segmenting data by progressive attribute selection"
- Exemplary techniques: Finding correlations, Attribute/variable selection; Tree induction
- We think of predictive modeling as "Supervised Segmentation"-how can we segment the population into groups that differ from each  other with  respect to some quantity of interest. -> Segment in  terms of something we want to predict or estimate
- Target can be  cast negatively (something we want to prevent) or positively
- "information is a quantity  that reduces uncertainty about something"
- "Finding informative attributes  also is the basis for *tree induction*"
- Model accuracy vs intelligibility  trade off (Best model vs one that makes sense)
- Models, Induction and Prediction
    - "Supervised  learning is model creation where the model describes a relationship between a set of select  variables (attrs or features) and a predefined  variable called the target variable"
    - Model induction = creation of models from data
        - Induction is a term from philosophy "to generalize from specific cases to general rules"
        - Induction algorithm/learner = procedure which creates the model from data
- Supervised Segmentation
    - How can we judge if a variable contains important information about the target variable? How much?
        - One method is "direct, multivariate supervised segmentation" (in classification)
            - When segmenting, we would prefer "pure" segments (each instance in the segment have same val for target), in the real world its not realistic though ("homogenous w.r.t. target variable")
            - 1. Attrs rarely split a group purely.
            - 2. Is it better when a group splits more broadly or more purely?
            - 3. Not all attrs are binary. How do we compare  these?
            - 4. Can you segment numeric attrs?
            - You solve these questions using a *purity measure* for classification problems
            - Most common splitting criterion is *information gain*. It is based on a purity measure called *entropy* (Shannon, 1948)
            - Explanation of entropy and information gain (p 51-55)
            - Information gain is not the correct splitting criterion for regression, variance is better for numeric but is technically a purity measure
- Example: Attr Selection with Information Gain
    - Discusses using information gain to reduce the entropy of a mushroom dataset
- Supervised Segmentation with Tree Structured Models
    - Classification/decision trees (p 63). Similar concept to trees in programming. The leaf nodes represent the class to predict. The decision points descend based on information gain. The first one after the root gives your best information gain.
    - *"Tree induction"* - How to create a tree structured model
    - Advantages to trees:
        - Easy to understand, simple to describe, easy to use, robust to common data problems, relatively efficient
    - Goal of tree: "Partition instances based on their attrs into subgroups that havee similar values for their target variables"
    - "Divide and conquer approach" - create purest subgroups possible
    - When do you stop splitting? When there are no more variables or you chose to stop earlier (simpler tree, Chapter 5)
- Visualizing Segmentations
    - Visualize how a classification tree partitions the instance space
    - Common form of instance space visualization is the scatter plot (p 70)
- Trees as Sets of Rules
    - Trees can be interpreted as logical statements using IF AND THEN syntax (p 71)
- Probability Estimation
    - Sometimes we need a more informative prediction than classification. For example is one prediction a more strong yes/no than another? How likely is someone to churn? This is whree probability estimation comes in
    - We would like each "segment to be assigned an estimate of the probability of membership in the different classes" (p 72)
    - *probability estimation tree*
    - frequency based estimate of class membership probability (extends the previously built up example of segmenting with trees)
        - Can lead to overfitting when there are not many examples at a leaf node
        - You can use smoothing "*Laplace correction*", which moderates influence of leafs with few instances
- Example: Addressing the Churn Problem with Tree Induction
    - Before building the model, check how good each variable is independetly with information gain
    - Apply classification tree alg
    - Notice that the order of the tree decisions does not necessarily match the initial information gain rankings. This is because "Nodes in the clf tree depend on the instances abovee them in the tree"
    - When do you stop building the tree?
        - Generality and overfitting come in here (CH 5)
        - Stop before these happen
    - Now that we have a model and we test it on some data, do we trust it? This is where *model evaluation* comes in (CH 7/8)
- Summary (p 78/79)
    - Information Gain
    - Entropy
    - Feature Selection
    - Segmenting
    - Trees and their history/implmentations
